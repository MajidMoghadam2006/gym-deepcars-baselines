Logging to ./logs/2019-05-23_17-17-18
-----------------------------------
| % time spent exploring  | 97    |
| episodes                | 100   |
| mean 100 episode reward | -93.3 |
| steps                   | 763   |
-----------------------------------
-----------------------------------
| % time spent exploring  | 95    |
| episodes                | 200   |
| mean 100 episode reward | -94.3 |
| steps                   | 1434  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 92    |
| episodes                | 300   |
| mean 100 episode reward | -93.4 |
| steps                   | 2194  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 90    |
| episodes                | 400   |
| mean 100 episode reward | -93.2 |
| steps                   | 2974  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 87    |
| episodes                | 500   |
| mean 100 episode reward | -93.6 |
| steps                   | 3714  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 85    |
| episodes                | 600   |
| mean 100 episode reward | -93.1 |
| steps                   | 4504  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 82    |
| episodes                | 700   |
| mean 100 episode reward | -93.4 |
| steps                   | 5262  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 79    |
| episodes                | 800   |
| mean 100 episode reward | -91.7 |
| steps                   | 6192  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 77    |
| episodes                | 900   |
| mean 100 episode reward | -92.7 |
| steps                   | 7019  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 74    |
| episodes                | 1000  |
| mean 100 episode reward | -93.9 |
| steps                   | 7732  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 72    |
| episodes                | 1100  |
| mean 100 episode reward | -92.7 |
| steps                   | 8564  |
-----------------------------------
-----------------------------------
| % time spent exploring  | 68    |
| episodes                | 1200  |
| mean 100 episode reward | -91.5 |
| steps                   | 9516  |
-----------------------------------
Saving model due to mean reward increase: None -> -91.0
-----------------------------------
| % time spent exploring  | 65    |
| episodes                | 1300  |
| mean 100 episode reward | -90.9 |
| steps                   | 10528 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 62    |
| episodes                | 1400  |
| mean 100 episode reward | -90.8 |
| steps                   | 11547 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 58    |
| episodes                | 1500  |
| mean 100 episode reward | -90.7 |
| steps                   | 12578 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 55    |
| episodes                | 1600  |
| mean 100 episode reward | -90.5 |
| steps                   | 13625 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 52    |
| episodes                | 1700  |
| mean 100 episode reward | -91.9 |
| steps                   | 14538 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 48    |
| episodes                | 1800  |
| mean 100 episode reward | -88.6 |
| steps                   | 15774 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 44    |
| episodes                | 1900  |
| mean 100 episode reward | -88.5 |
| steps                   | 17022 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 40    |
| episodes                | 2000  |
| mean 100 episode reward | -88.1 |
| steps                   | 18309 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 35    |
| episodes                | 2100  |
| mean 100 episode reward | -86.2 |
| steps                   | 19789 |
-----------------------------------
Saving model due to mean reward increase: -91.0 -> -85.5
-----------------------------------
| % time spent exploring  | 29    |
| episodes                | 2200  |
| mean 100 episode reward | -83.8 |
| steps                   | 21507 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 22    |
| episodes                | 2300  |
| mean 100 episode reward | -78.2 |
| steps                   | 23792 |
-----------------------------------
-----------------------------------
| % time spent exploring  | 10    |
| episodes                | 2400  |
| mean 100 episode reward | -65.8 |
| steps                   | 27317 |
-----------------------------------
Saving model due to mean reward increase: -85.5 -> -50.400001525878906
Saving model due to mean reward increase: -50.400001525878906 -> 39.5
Saving model due to mean reward increase: 39.5 -> 121.0999984741211
-----------------------------------
| % time spent exploring  | 2     |
| episodes                | 2500  |
| mean 100 episode reward | 127.5 |
| steps                   | 50168 |
-----------------------------------
Saving model due to mean reward increase: 121.0999984741211 -> 205.3000030517578
-----------------------------------
| % time spent exploring  | 2     |
| episodes                | 2600  |
| mean 100 episode reward | 188.7 |
| steps                   | 79142 |
-----------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 2700   |
| mean 100 episode reward | 142.8  |
| steps                   | 103526 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 2800   |
| mean 100 episode reward | 162.1  |
| steps                   | 129833 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 2900   |
| mean 100 episode reward | 138.7  |
| steps                   | 153799 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 3000   |
| mean 100 episode reward | 160.3  |
| steps                   | 179928 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 3100   |
| mean 100 episode reward | 135.0  |
| steps                   | 203533 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 3200   |
| mean 100 episode reward | 166.8  |
| steps                   | 230315 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 3300   |
| mean 100 episode reward | 140.1  |
| steps                   | 254421 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 3400   |
| mean 100 episode reward | 104.3  |
| steps                   | 274951 |
------------------------------------
------------------------------------
| % time spent exploring  | 2      |
| episodes                | 3500   |
| mean 100 episode reward | 116.0  |
| steps                   | 296646 |
------------------------------------
Restored model with mean reward: 205.3000030517578
------------------Inpute arguments------------------
Namespace(alg='deepq', env='DeepCars-v2', env_type=None, extra_import=None, gamestate=None, network='deep_mlp2', num_env=None, num_timesteps=300000.0, play=False, reward_scale=1.0, save_path=None, save_video_interval=0, save_video_length=200, seed=None)
{}
----------------------------------------------------
